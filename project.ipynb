{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d9173e-2898-4604-a9b3-42dcf76701f1",
   "metadata": {},
   "source": [
    "# Environment Setup and Configuration\n",
    "\n",
    "\n",
    "### üöÄ The Pre-Flight Checklist**\n",
    "\n",
    "Alright, before we get to the cool machine learning magic, we have to do the boring-but-brilliant setup.\n",
    "\n",
    "This next cell is our project's bouncer. It checks our Python environment to see what tools we're working with and peeks in the back to see if a GPU is ready to do the heavy lifting. It then pulls up our \"recipe book\" of settings and keeps a to-do list to track our progress.\n",
    "\n",
    "Basically, it makes sure all systems are go before we launch. If it runs without a peep, we're ready for the fun stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1162533-dd85-4082-a425-5af5370309e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment & Library Check ---\n",
      "‚úÖ Environment check complete.\n",
      "‚úÖ Configuration initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import importlib.util\n",
    "import importlib.metadata\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Global pipeline state management\n",
    "@dataclass\n",
    "class PipelineState:\n",
    "    valid: bool = True\n",
    "    skip_reason: str = \"\"\n",
    "    data_loaded: bool = False\n",
    "    model_trained: bool = False\n",
    "    predictions_generated: bool = False\n",
    "    \n",
    "    def invalidate(self, reason: str):\n",
    "        self.valid = False\n",
    "        self.skip_reason = reason\n",
    "        print(f\"‚ö†Ô∏è Pipeline halted: {reason}\")\n",
    "\n",
    "# Initialize global pipeline state\n",
    "pipeline_state = PipelineState()\n",
    "\n",
    "def env_check():\n",
    "    \"\"\"\n",
    "    Checks the user's environment for required and optional packages,\n",
    "    and sets global flags to control the notebook's behavior.\n",
    "    \"\"\"\n",
    "    specs = {\n",
    "        # Core\n",
    "        \"torch\": \"2.0.0\", \"numpy\": \"1.23.5\", \"pandas\": \"2.0.0\", \"scipy\": \"1.10.0\",\n",
    "        # Optional ML/DL\n",
    "        \"transformers\": \"4.30.0\", \"datasets\": \"2.14.0\", \"accelerate\": \"0.21.0\",\n",
    "        \"bitsandbytes\": \"0.41.0\", \"sentence-transformers\": \"2.2.2\",\n",
    "        \"tqdm\": \"4.64.1\", \"sklearn\": \"1.2.2\",\n",
    "        # Optional Viz/UI\n",
    "        \"matplotlib\": \"3.7.1\", \"ipywidgets\": \"8.0.4\",\n",
    "        # Less common but useful\n",
    "        \"holidays\": \"0.29\", \"timm\": \"0.9.5\", \"opencv-python\": \"4.8.0.74\",\n",
    "        \"pytorch_lightning\": \"2.0.0\"\n",
    "    }\n",
    "\n",
    "    print(\"--- Environment & Library Check ---\")\n",
    "    detected_versions = {}\n",
    "    flags = {}\n",
    "    optional_installs = []\n",
    "\n",
    "    for pkg, min_version in specs.items():\n",
    "        try:\n",
    "            version = importlib.metadata.version(pkg)\n",
    "            detected_versions[pkg] = version\n",
    "            flags[f\"USE_{pkg.upper().replace('-', '_')}\"] = True\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            flags[f\"USE_{pkg.upper().replace('-', '_')}\"] = False\n",
    "            if pkg in [\"transformers\", \"accelerate\", \"bitsandbytes\", \"ipywidgets\", \"holidays\", \"tqdm\", \"scikit-learn\"]:\n",
    "                install_name = \"scikit-learn\" if pkg == \"sklearn\" else pkg\n",
    "                optional_installs.append(f\"pip install {install_name}\")\n",
    "\n",
    "    use_torch = flags.get('USE_TORCH', False)\n",
    "    if use_torch:\n",
    "        import torch\n",
    "        flags['CUDA_AVAILABLE'] = torch.cuda.is_available()\n",
    "    else:\n",
    "        flags['CUDA_AVAILABLE'] = False\n",
    "    \n",
    "    print(\"‚úÖ Environment check complete.\")\n",
    "    if optional_installs:\n",
    "        print(\"üí° Optional libraries not found. For enhanced features, consider installing:\")\n",
    "        for cmd in set(optional_installs):\n",
    "            print(f\"   {cmd}\")\n",
    "    return flags\n",
    "\n",
    "@dataclass\n",
    "class ProjectConfig:\n",
    "    env_flags: Dict[str, bool] = field(default_factory=dict)\n",
    "    DATA_PATH: str = \"./data_cache\"\n",
    "    TARGET_COLUMN: str = \"target_load\"\n",
    "    TIME_COLUMN: str = \"utc_timestamp\"\n",
    "    GRANULARITY: str = \"H\"\n",
    "    MODEL_NAME: str = \"tcn_transformer_hybrid\"\n",
    "    LOOKBACK_WINDOW: int = 72\n",
    "    FORECAST_HORIZON: int = 12\n",
    "    QUANTILES: List[float] = field(default_factory=lambda: [0.1, 0.5, 0.9])\n",
    "    BATCH_SIZE: int = 64\n",
    "    EPOCHS: int = 10\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    PATIENCE: int = 3\n",
    "    USE_AMP: bool = True\n",
    "    MODEL_SAVE_PATH: str = \"./models\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.DATA_PATH, exist_ok=True)\n",
    "        os.makedirs(self.MODEL_SAVE_PATH, exist_ok=True)\n",
    "        self.USE_AMP = self.env_flags.get('CUDA_AVAILABLE', False)\n",
    "\n",
    "def get_progress_bar():\n",
    "    \"\"\"\n",
    "    Returns the tqdm progress bar if available, otherwise a dummy iterator.\n",
    "    \"\"\"\n",
    "    if config.env_flags.get('USE_TQDM', False):\n",
    "        from tqdm.autonotebook import tqdm\n",
    "        return tqdm\n",
    "    else:\n",
    "        # Fallback to a dummy iterator that just returns the object\n",
    "        return lambda x, *args, **kwargs: x\n",
    "\n",
    "# Run the check and create the config object\n",
    "try:\n",
    "    ENV_FLAGS = env_check()\n",
    "    config = ProjectConfig(env_flags=ENV_FLAGS)\n",
    "    print(\"‚úÖ Configuration initialized successfully.\")\n",
    "except Exception as e:\n",
    "    pipeline_state.invalidate(f\"Environment setup failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4c279-b4dd-4323-ae11-add233697bd0",
   "metadata": {},
   "source": [
    "# Data Access Layer\n",
    "### üíæ Time to Get the Data\n",
    "\n",
    "Alright, let's grab some data. This part is all about fetching our datasets from the internet.\n",
    "\n",
    "* This block is smart about it, it will only download the data once and then save a local copy (a \"cache\"). The next time you run it, it'll be lightning-fast because it'll just load the data from your computer. We're also doing a quick cleanup to make sure the column names are consistent, so we don't have to worry about it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedfbeb7-44ee-4746-8992-6b95aa36dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Access Layer ---\n",
      "‚úÖ Loaded cached OPSD data\n",
      "\n",
      "OPSD Data Sample (standardized):\n",
      "                     target_load\n",
      "Date                            \n",
      "2006-01-01 00:00:00     1069.184\n",
      "2006-01-01 01:00:00     1069.184\n",
      "2006-01-01 02:00:00     1069.184\n",
      "2006-01-01 03:00:00     1069.184\n",
      "2006-01-01 04:00:00     1069.184\n",
      "Data shape: (105169, 1)\n",
      "Date range: 2006-01-01 00:00:00 to 2017-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def check_pipeline_state(operation_name):\n",
    "    \"\"\"Check if pipeline is valid before proceeding with operation\"\"\"\n",
    "    if not pipeline_state.valid:\n",
    "        print(f\"‚ö†Ô∏è Skipping {operation_name}: {pipeline_state.skip_reason}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def load_opsd(cache_path=None, resample_freq=None):\n",
    "    \"\"\"\n",
    "    Loads the Open Power System Data (OPSD) for Germany.\n",
    "    Caches the data with its original 'Consumption' column name.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"OPSD data loading\"):\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if cache_path is None:\n",
    "        cache_path = config.DATA_PATH\n",
    "    if resample_freq is None:\n",
    "        resample_freq = config.GRANULARITY\n",
    "        \n",
    "    file_path = os.path.join(cache_path, f\"opsd_germany_{resample_freq}_original.pkl\")\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            print(\"‚úÖ Loaded cached OPSD data\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load cached data: {e}\")\n",
    "    \n",
    "    print(\"‚¨á Downloading OPSD data...\")\n",
    "    url = 'https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(url, index_col='Date', parse_dates=True)\n",
    "        if 'Consumption' not in df.columns:\n",
    "            raise ValueError(\"Expected 'Consumption' column not found in OPSD data\")\n",
    "        \n",
    "        df = df[['Consumption']].resample(resample_freq).ffill()\n",
    "        df.to_pickle(file_path)\n",
    "        print(f\"üíæ Cached OPSD data to {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error downloading or processing OPSD data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_uci_household(cache_path=None, resample_freq=None):\n",
    "    \"\"\"\n",
    "    Loads the UCI Individual household electric power consumption dataset.\n",
    "    Caches the data with its original 'Global_active_power' column name.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"UCI household data loading\"):\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if cache_path is None:\n",
    "        cache_path = config.DATA_PATH\n",
    "    if resample_freq is None:\n",
    "        resample_freq = config.GRANULARITY\n",
    "        \n",
    "    file_path = os.path.join(cache_path, f\"uci_household_{resample_freq}_original.pkl\")\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            print(\"‚úÖ Loaded cached UCI household data\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load cached data: {e}\")\n",
    "    \n",
    "    print(\"‚¨á Downloading and processing UCI Household data...\")\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip'\n",
    "    try:\n",
    "        df = pd.read_csv(url, sep=';', compression='zip', low_memory=False, \n",
    "                        na_values=['?'], parse_dates={'datetime': ['Date', 'Time']}, \n",
    "                        infer_datetime_format=True, dayfirst=True)\n",
    "        df = df.set_index('datetime')\n",
    "        if 'Global_active_power' not in df.columns:\n",
    "            raise ValueError(\"Expected 'Global_active_power' column not found in UCI data\")\n",
    "            \n",
    "        df = df[['Global_active_power']].astype(float)\n",
    "        df = df.resample(resample_freq).sum()\n",
    "        df.to_pickle(file_path)\n",
    "        print(f\"üíæ Cached UCI data to {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error downloading or processing UCI household data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load and standardize data\n",
    "if check_pipeline_state(\"Data loading\"):\n",
    "    print(\"--- Data Access Layer ---\")\n",
    "    \n",
    "    # Load the raw data (default to OPSD)\n",
    "    try:\n",
    "        df_opsd_raw = load_opsd()\n",
    "        \n",
    "        if not df_opsd_raw.empty and 'Consumption' in df_opsd_raw.columns:\n",
    "            # Standardize the column name\n",
    "            df_opsd = df_opsd_raw.rename(columns={'Consumption': config.TARGET_COLUMN})\n",
    "            pipeline_state.data_loaded = True\n",
    "            \n",
    "            print(\"\\nOPSD Data Sample (standardized):\")\n",
    "            print(df_opsd.head())\n",
    "            print(f\"Data shape: {df_opsd.shape}\")\n",
    "            print(f\"Date range: {df_opsd.index.min()} to {df_opsd.index.max()}\")\n",
    "        else:\n",
    "            pipeline_state.invalidate(\"Failed to load OPSD data or missing expected columns\")\n",
    "            df_opsd = pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error in data loading process: {str(e)}\")\n",
    "        df_opsd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79901a-94dc-49ca-b4fa-f5b4c231d776",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "### üí™ Whipping the Data into Shape\n",
    "\n",
    "* Raw data is a bit of a wild beast. It's messy, has weird gaps, and sometimes throws out crazy, unbelievable numbers. You can't just feed that to a sophisticated model; it would get indigestion.\n",
    "\n",
    "* This next cell is the data's personal trainer. It forces our dataset to do some cleanup:\n",
    "\n",
    "* Fills in the Blanks: Patches up any missing values.\n",
    "\n",
    "* Trims the Fat: Clips the extreme, wild outliers that don't make sense.\n",
    "\n",
    "* Puts Everyone on the Same Scale: Resizes all the numbers so they live in the same neighborhood.\n",
    "\n",
    "This way, our model can focus on the real patterns instead of getting distracted by a few loud, obnoxious numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9429622-9634-47d5-b504-c2262c1862a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Data ---\n",
      "   - Clipped target values to 1st and 99th percentiles.\n",
      "   - Scaled data manually (NumPy/Pandas fallback).\n",
      "\n",
      "Preprocessed Data Sample:\n",
      "                     target_load  target_load_scaled\n",
      "Date                                                \n",
      "2006-01-01 00:00:00     1069.184           -1.637032\n",
      "2006-01-01 01:00:00     1069.184           -1.637032\n",
      "2006-01-01 02:00:00     1069.184           -1.637032\n",
      "2006-01-01 03:00:00     1069.184           -1.637032\n",
      "2006-01-01 04:00:00     1069.184           -1.637032\n",
      "Processed data shape: (105169, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check if scikit-learn is available for the scaler\n",
    "if config.env_flags.get('USE_SKLEARN', False):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_for_model(df, target_col):\n",
    "    \"\"\"\n",
    "    Cleans, scales, and prepares the dataframe for model input.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Data preprocessing\"):\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    if df.empty:\n",
    "        pipeline_state.invalidate(\"Cannot preprocess empty dataframe\")\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    if target_col not in df.columns:\n",
    "        pipeline_state.invalidate(f\"Target column '{target_col}' not found in dataframe\")\n",
    "        return pd.DataFrame(), None\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Preprocessing Data ---\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        original_rows = len(df_processed)\n",
    "        df_processed = df_processed.dropna(subset=[target_col])\n",
    "        if len(df_processed) < original_rows:\n",
    "            print(f\"   - Dropped {original_rows - len(df_processed)} rows with missing target.\")\n",
    "        \n",
    "        # Check if we have enough data after dropping missing values\n",
    "        if len(df_processed) < config.LOOKBACK_WINDOW + config.FORECAST_HORIZON:\n",
    "            pipeline_state.invalidate(f\"Insufficient data after preprocessing: {len(df_processed)} rows, need at least {config.LOOKBACK_WINDOW + config.FORECAST_HORIZON}\")\n",
    "            return pd.DataFrame(), None\n",
    "        \n",
    "        # Use time-based interpolation for missing values in other columns if any\n",
    "        df_processed = df_processed.interpolate(method='time')\n",
    "        \n",
    "        # 2. Outlier handling (simple clipping)\n",
    "        q1 = df_processed[target_col].quantile(0.01)\n",
    "        q99 = df_processed[target_col].quantile(0.99)\n",
    "        df_processed[target_col] = np.clip(df_processed[target_col], q1, q99)\n",
    "        print(\"   - Clipped target values to 1st and 99th percentiles.\")\n",
    "\n",
    "        # 3. Scaling\n",
    "        if config.env_flags.get('USE_SKLEARN', False):\n",
    "            scaler = StandardScaler()\n",
    "            df_processed[f\"{target_col}_scaled\"] = scaler.fit_transform(df_processed[[target_col]])\n",
    "            print(\"   - Scaled data using StandardScaler.\")\n",
    "        else:\n",
    "            # Fallback to manual scaling\n",
    "            mean = df_processed[target_col].mean()\n",
    "            std = df_processed[target_col].std()\n",
    "            if std == 0:\n",
    "                pipeline_state.invalidate(\"Target column has zero standard deviation - cannot scale\")\n",
    "                return pd.DataFrame(), None\n",
    "            df_processed[f\"{target_col}_scaled\"] = (df_processed[target_col] - mean) / std\n",
    "            scaler = {'mean': mean, 'std': std}  # Save for inverse transform\n",
    "            print(\"   - Scaled data manually (NumPy/Pandas fallback).\")\n",
    "        \n",
    "        return df_processed, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error during preprocessing: {str(e)}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "# Execute preprocessing\n",
    "if check_pipeline_state(\"Preprocessing execution\") and pipeline_state.data_loaded:\n",
    "    try:\n",
    "        data_df, scaler = preprocess_for_model(df_opsd.copy(), config.TARGET_COLUMN)\n",
    "        \n",
    "        if not data_df.empty and scaler is not None:\n",
    "            print(\"\\nPreprocessed Data Sample:\")\n",
    "            print(data_df.head())\n",
    "            print(f\"Processed data shape: {data_df.shape}\")\n",
    "        else:\n",
    "            pipeline_state.invalidate(\"Preprocessing failed to produce valid output\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Preprocessing execution failed: {str(e)}\")\n",
    "        data_df, scaler = pd.DataFrame(), None\n",
    "else:\n",
    "    data_df, scaler = pd.DataFrame(), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08300267-df3c-4167-8256-233c10d71f5b",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "### üïµÔ∏è‚Äç‚ôÇÔ∏è Giving Our Data Superpowers\n",
    "\n",
    "* Our data is clean, but it's a bit plain. A machine learning model needs more than just a single column of numbers; it needs clues to find patterns. This process, called feature engineering, is like giving our data a detective's toolkit.\n",
    "\n",
    "* What Clues Are We Creating?\n",
    "We're adding several new columns to give the model more context about each data point:\n",
    "\n",
    "* Time Features: We're telling the model the hour, day of the week, and month for each entry. This helps it learn daily, weekly, and yearly cycles.\n",
    "\n",
    "* Holiday Flags: We mark down whether a day is a holiday, since people's behavior (and energy use) changes dramatically on those days.\n",
    "\n",
    "* Lag Features: We add columns showing what the value was 24 hours ago, 48 hours ago, and a week ago. This gives the model a sense of recent history.\n",
    "\n",
    "* Rolling Averages: We calculate the average and standard deviation over the last day and week. This tells the model if things have been trending up, down, or have been unusually volatile.\n",
    "\n",
    "* Fourier Terms: These are fancy sin/cos waves that help the model understand the smooth, cyclical nature of a year, preventing it from thinking December 31st is wildly different from January 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c471fc-6f35-467f-bcc8-0c51eb176cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Features ---\n",
      "   - Added time-based features (hour, dayofweek, etc.).\n",
      "   - Added holiday features using 'holidays' library.\n",
      "   - Added lagged features (24h, 48h, 168h).\n",
      "   - Added rolling window features (mean/std over 24h, 168h).\n",
      "   - Added Fourier terms for yearly seasonality.\n",
      "   - Dropped 168 rows with NaN values from feature engineering.\n",
      "   - Final dataset shape: (105001, 17)\n",
      "\n",
      "Data Sample with Features:\n",
      "                     target_load  target_load_scaled  hour  dayofweek  month  \\\n",
      "Date                                                                           \n",
      "2006-01-08 00:00:00     1207.985           -0.794307     0          6      1   \n",
      "2006-01-08 01:00:00     1207.985           -0.794307     1          6      1   \n",
      "2006-01-08 02:00:00     1207.985           -0.794307     2          6      1   \n",
      "2006-01-08 03:00:00     1207.985           -0.794307     3          6      1   \n",
      "2006-01-08 04:00:00     1207.985           -0.794307     4          6      1   \n",
      "\n",
      "                     year  dayofyear  is_holiday    lag_24    lag_48  \\\n",
      "Date                                                                   \n",
      "2006-01-08 00:00:00  2006          8           0 -0.233899  0.392312   \n",
      "2006-01-08 01:00:00  2006          8           0 -0.233899  0.392312   \n",
      "2006-01-08 02:00:00  2006          8           0 -0.233899  0.392312   \n",
      "2006-01-08 03:00:00  2006          8           0 -0.233899  0.392312   \n",
      "2006-01-08 04:00:00  2006          8           0 -0.233899  0.392312   \n",
      "\n",
      "                      lag_168  rolling_mean_24  rolling_std_24  \\\n",
      "Date                                                             \n",
      "2006-01-08 00:00:00 -1.637032        -0.233899        0.000000   \n",
      "2006-01-08 01:00:00 -1.637032        -0.257249        0.114393   \n",
      "2006-01-08 02:00:00 -1.637032        -0.280600        0.158220   \n",
      "2006-01-08 03:00:00 -1.637032        -0.303950        0.189324   \n",
      "2006-01-08 04:00:00 -1.637032        -0.327300        0.213344   \n",
      "\n",
      "                     rolling_mean_168  rolling_std_168  sin_dayofyear  \\\n",
      "Date                                                                    \n",
      "2006-01-08 00:00:00          0.137580         0.798692       0.137185   \n",
      "2006-01-08 01:00:00          0.142596         0.790079       0.137185   \n",
      "2006-01-08 02:00:00          0.147612         0.781340       0.137185   \n",
      "2006-01-08 03:00:00          0.152629         0.772468       0.137185   \n",
      "2006-01-08 04:00:00          0.157645         0.763461       0.137185   \n",
      "\n",
      "                     cos_dayofyear  \n",
      "Date                                \n",
      "2006-01-08 00:00:00       0.990545  \n",
      "2006-01-08 01:00:00       0.990545  \n",
      "2006-01-08 02:00:00       0.990545  \n",
      "2006-01-08 03:00:00       0.990545  \n",
      "2006-01-08 04:00:00       0.990545  \n",
      "Feature columns: ['target_load', 'target_load_scaled', 'hour', 'dayofweek', 'month', 'year', 'dayofyear', 'is_holiday', 'lag_24', 'lag_48', 'lag_168', 'rolling_mean_24', 'rolling_std_24', 'rolling_mean_168', 'rolling_std_168', 'sin_dayofyear', 'cos_dayofyear']\n"
     ]
    }
   ],
   "source": [
    "if config.env_flags.get('USE_HOLIDAYS', False):\n",
    "    import holidays\n",
    "\n",
    "def build_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Enriches the dataframe with time-based, holiday, and lagged features.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Feature engineering\"):\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if df.empty:\n",
    "        pipeline_state.invalidate(\"Cannot build features from empty dataframe\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if target_col not in df.columns:\n",
    "        pipeline_state.invalidate(f\"Target column '{target_col}' not found for feature engineering\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Building Features ---\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original data\n",
    "        df_featured = df.copy()\n",
    "        \n",
    "        # Time-based features\n",
    "        df_featured['hour'] = df_featured.index.hour\n",
    "        df_featured['dayofweek'] = df_featured.index.dayofweek\n",
    "        df_featured['month'] = df_featured.index.month\n",
    "        df_featured['year'] = df_featured.index.year\n",
    "        df_featured['dayofyear'] = df_featured.index.dayofyear\n",
    "        print(\"   - Added time-based features (hour, dayofweek, etc.).\")\n",
    "\n",
    "        # Holiday features\n",
    "        if config.env_flags.get('USE_HOLIDAYS', False):\n",
    "            try:\n",
    "                de_holidays = holidays.Germany()\n",
    "                df_featured['is_holiday'] = df_featured.index.map(lambda x: 1 if x in de_holidays else 0)\n",
    "                print(\"   - Added holiday features using 'holidays' library.\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - Warning: Could not use holidays library ({e}), using fallback.\")\n",
    "                df_featured['is_holiday'] = ((df_featured.index.month == 12) & (df_featured.index.day.isin([24, 25, 26, 31]))) | \\\n",
    "                                           ((df_featured.index.month == 1) & (df_featured.index.day == 1))\n",
    "                df_featured['is_holiday'] = df_featured['is_holiday'].astype(int)\n",
    "                print(\"   - Added simple holiday fallback (e.g., Christmas/New Year).\")\n",
    "        else:\n",
    "            # Simple fallback for major holidays\n",
    "            df_featured['is_holiday'] = ((df_featured.index.month == 12) & (df_featured.index.day.isin([24, 25, 26, 31]))) | \\\n",
    "                                       ((df_featured.index.month == 1) & (df_featured.index.day == 1))\n",
    "            df_featured['is_holiday'] = df_featured['is_holiday'].astype(int)\n",
    "            print(\"   - Added simple holiday fallback (e.g., Christmas/New Year).\")\n",
    "\n",
    "        # Lagged features\n",
    "        for lag in [24, 48, 168]:  # 1 day, 2 days, 1 week ago\n",
    "            df_featured[f'lag_{lag}'] = df_featured[target_col].shift(lag)\n",
    "        print(\"   - Added lagged features (24h, 48h, 168h).\")\n",
    "\n",
    "        # Rolling window features\n",
    "        for window in [24, 168]:\n",
    "            df_featured[f'rolling_mean_{window}'] = df_featured[target_col].shift(1).rolling(window=window).mean()\n",
    "            df_featured[f'rolling_std_{window}'] = df_featured[target_col].shift(1).rolling(window=window).std()\n",
    "        print(\"   - Added rolling window features (mean/std over 24h, 168h).\")\n",
    "\n",
    "        # Fourier terms for seasonality\n",
    "        df_featured['sin_dayofyear'] = np.sin(2 * np.pi * df_featured['dayofyear'] / 365.25)\n",
    "        df_featured['cos_dayofyear'] = np.cos(2 * np.pi * df_featured['dayofyear'] / 365.25)\n",
    "        print(\"   - Added Fourier terms for yearly seasonality.\")\n",
    "\n",
    "        # Drop rows with NaNs created by lags/rolling windows\n",
    "        initial_rows = len(df_featured)\n",
    "        df_featured = df_featured.dropna()\n",
    "        dropped_rows = initial_rows - len(df_featured)\n",
    "        \n",
    "        if dropped_rows > 0:\n",
    "            print(f\"   - Dropped {dropped_rows} rows with NaN values from feature engineering.\")\n",
    "        \n",
    "        # Check if we still have enough data\n",
    "        min_required = config.LOOKBACK_WINDOW + config.FORECAST_HORIZON\n",
    "        if len(df_featured) < min_required:\n",
    "            pipeline_state.invalidate(f\"Insufficient data after feature engineering: {len(df_featured)} rows, need at least {min_required}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"   - Final dataset shape: {df_featured.shape}\")\n",
    "        return df_featured\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error during feature engineering: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute feature engineering\n",
    "if check_pipeline_state(\"Feature engineering execution\") and not data_df.empty:\n",
    "    try:\n",
    "        featured_df = build_features(data_df, f\"{config.TARGET_COLUMN}_scaled\")\n",
    "        \n",
    "        if not featured_df.empty:\n",
    "            print(\"\\nData Sample with Features:\")\n",
    "            print(featured_df.head())\n",
    "            print(f\"Feature columns: {list(featured_df.columns)}\")\n",
    "        else:\n",
    "            pipeline_state.invalidate(\"Feature engineering failed to produce valid output\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Feature engineering execution failed: {str(e)}\")\n",
    "        featured_df = pd.DataFrame()\n",
    "else:\n",
    "    featured_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3789b-60d7-442c-9189-5b7874328546",
   "metadata": {},
   "source": [
    "# Splitting and Walk-Forward Validation\n",
    "\n",
    "### ‚è≥ The Time Traveler's Test: Splitting Our Data\n",
    "\n",
    "When you're predicting the future, you can't cheat.\n",
    "A normal train_test_split would shuffle our data, letting the model peek at future events to \"predict\" the past. That's like giving it a crystal ball‚Äîit'll look like a genius, but it's completely useless in the real world.\n",
    "\n",
    "The Fair Fight: Walk-Forward Validation\n",
    "Instead, we use a much smarter method called Walk-Forward Validation. It mimics reality by slicing up our timeline into a series of \"past vs. future\" challenges.\n",
    "\n",
    "It works like this:\n",
    "\n",
    "* Train the model on data from the past (e.g., Week 1-4).\n",
    "\n",
    "* Test it by having it predict the immediate future (e.g., Week 5).\n",
    "\n",
    "Then, we \"walk forward\" and repeat: train on Week 1-5, test on Week 6.\n",
    "\n",
    "Our walk_forward_split function is the machine that creates these realistic training and testing windows. This ensures our model is judged on its ability to genuinely forecast, with no cheating allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e16500-f8e7-4485-8f98-0d513bcef986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Walk-Forward Validation Splits ---\n",
      "Created 5 valid splits\n",
      "Split 1:\n",
      "  Train: indices from 104221 to 104940 (length 720)\n",
      "  Test:  indices from 104941 to 104952 (length 12)\n",
      "Split 2:\n",
      "  Train: indices from 104233 to 104952 (length 720)\n",
      "  Test:  indices from 104953 to 104964 (length 12)\n",
      "Split 3:\n",
      "  Train: indices from 104245 to 104964 (length 720)\n",
      "  Test:  indices from 104965 to 104976 (length 12)\n",
      "  ... and 2 more splits\n",
      "‚úÖ Splits validated successfully\n"
     ]
    }
   ],
   "source": [
    "def walk_forward_split(data, n_splits=5, lookback=None, horizon=None):\n",
    "    \"\"\"\n",
    "    Generates indices for walk-forward validation.\n",
    "    Returns None if data is insufficient.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Walk-forward split\"):\n",
    "        return None\n",
    "        \n",
    "    if data.empty:\n",
    "        pipeline_state.invalidate(\"Cannot create splits from empty data\")\n",
    "        return None\n",
    "        \n",
    "    if lookback is None:\n",
    "        lookback = config.LOOKBACK_WINDOW\n",
    "    if horizon is None:\n",
    "        horizon = config.FORECAST_HORIZON\n",
    "    \n",
    "    try:\n",
    "        total_samples = len(data)\n",
    "        min_required = lookback + horizon\n",
    "        \n",
    "        # Check if we have enough data for even one split\n",
    "        if total_samples < min_required:\n",
    "            pipeline_state.invalidate(f\"Insufficient data for walk-forward split: {total_samples} samples, need at least {min_required}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate maximum possible splits\n",
    "        max_possible_splits = total_samples - min_required + 1\n",
    "        actual_splits = min(n_splits, max_possible_splits)\n",
    "        \n",
    "        if actual_splits < n_splits:\n",
    "            print(f\"   - Requested {n_splits} splits, but only {actual_splits} possible with available data\")\n",
    "        \n",
    "        # Start splitting from the end of the data\n",
    "        split_points = np.linspace(\n",
    "            total_samples - actual_splits * horizon, \n",
    "            total_samples - horizon, \n",
    "            actual_splits, \n",
    "            dtype=int\n",
    "        )\n",
    "\n",
    "        valid_splits = []\n",
    "        for split_point in split_points:\n",
    "            train_end = split_point\n",
    "            train_start = max(0, train_end - lookback * 10)  # Limit training history size for efficiency\n",
    "            test_start = train_end\n",
    "            test_end = test_start + horizon\n",
    "            \n",
    "            if test_end <= total_samples and train_end > train_start:\n",
    "                train_indices = np.arange(train_start, train_end)\n",
    "                test_indices = np.arange(test_start, test_end)\n",
    "                valid_splits.append((train_indices, test_indices))\n",
    "        \n",
    "        if not valid_splits:\n",
    "            pipeline_state.invalidate(\"No valid splits could be created\")\n",
    "            return None\n",
    "            \n",
    "        return valid_splits\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error creating walk-forward splits: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create splits and validate\n",
    "if check_pipeline_state(\"Split creation\") and not featured_df.empty:\n",
    "    print(\"\\n--- Walk-Forward Validation Splits ---\")\n",
    "    \n",
    "    try:\n",
    "        splits = walk_forward_split(featured_df)\n",
    "        \n",
    "        if splits is not None and len(splits) > 0:\n",
    "            print(f\"Created {len(splits)} valid splits\")\n",
    "            \n",
    "            # Show first few splits for inspection\n",
    "            for i, (train_indices, test_indices) in enumerate(splits[:3]):  # Show first 3 splits\n",
    "                print(f\"Split {i+1}:\")\n",
    "                print(f\"  Train: indices from {train_indices.min()} to {train_indices.max()} (length {len(train_indices)})\")\n",
    "                print(f\"  Test:  indices from {test_indices.min()} to {test_indices.max()} (length {len(test_indices)})\")\n",
    "            \n",
    "            if len(splits) > 3:\n",
    "                print(f\"  ... and {len(splits) - 3} more splits\")\n",
    "                \n",
    "            # Store the first split for model training\n",
    "            train_indices, test_indices = splits[0]\n",
    "            \n",
    "            # Validate split sizes\n",
    "            if len(train_indices) < config.LOOKBACK_WINDOW:\n",
    "                pipeline_state.invalidate(f\"Training split too small: {len(train_indices)} < {config.LOOKBACK_WINDOW}\")\n",
    "            elif len(test_indices) < config.FORECAST_HORIZON:\n",
    "                pipeline_state.invalidate(f\"Test split too small: {len(test_indices)} < {config.FORECAST_HORIZON}\")\n",
    "            else:\n",
    "                print(\"‚úÖ Splits validated successfully\")\n",
    "                \n",
    "        else:\n",
    "            pipeline_state.invalidate(\"No valid splits were created\")\n",
    "            train_indices, test_indices = None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error during split creation: {str(e)}\")\n",
    "        splits = None\n",
    "        train_indices, test_indices = None, None\n",
    "else:\n",
    "    splits = None\n",
    "    train_indices, test_indices = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cc83d-a23c-42e6-b5e2-c70eef6148a2",
   "metadata": {},
   "source": [
    "# TCN + Transformer Hybrid Model \n",
    "### üß† Building the Brain: A Hybrid TCN-Transformer\n",
    "\n",
    "It's time to build our forecasting model! We're not just picking one off the shelf; we're building a powerful hybrid that combines the best of two worlds. Think of it as a super-team-up between two different kinds of geniuses.\n",
    "\n",
    "**Meet the Team**\n",
    "Our model, the TCNTransformerHybrid, is made of two main parts that work together:\n",
    "\n",
    "* **The Local Expert (TCN):** First up is the Temporal Convolutional Network (TCN). This part is brilliant at spotting local patterns and short-term trends. It scans the sequence of data like a detective looking for immediate clues and textures in the timeline.\n",
    "\n",
    "* **The Big Picture Thinker (Transformer):** The features identified by the TCN are then passed to a Transformer Encoder. The Transformer is a master of understanding context and relationships between points far apart in time. It figures out which past events are truly important for predicting the future, no matter how long ago they happened.\n",
    "\n",
    "By combining them, the TCN handles the detailed, local feature extraction, and the Transformer handles the high-level, long-range dependencies. This powerful duo gives us a robust model capable of capturing complex time-series dynamics. Finally, a simple Linear layer takes this rich understanding and shapes it into our final forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85dd05c4-b183-46b8-a143-87ec04c232ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TCN+Transformer Hybrid Model Definition ---\n",
      "Input features: 15\n",
      "Feature columns: ['hour', 'dayofweek', 'month', 'year', 'dayofyear']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panchani.d/.conda/envs/pytorch_env/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TCN+Transformer Hybrid model initialized successfully.\n",
      "Model parameters: 572,516\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNTransformerHybrid(nn.Module):\n",
    "    def __init__(self, input_size, output_size, horizon, num_quantiles, tcn_channels=[32, 64], d_model=64, nhead=4, num_layers=2):\n",
    "        super(TCNTransformerHybrid, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, tcn_channels)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=tcn_channels[-1], nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.horizon = horizon\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "        # Flattened output for horizon * quantiles\n",
    "        self.fc = nn.Linear(tcn_channels[-1], output_size * horizon * num_quantiles)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        x = x.permute(0, 2, 1)  # TCN expects (batch, channels, seq_len)\n",
    "        tcn_out = self.tcn(x)\n",
    "        tcn_out = tcn_out.permute(0, 2, 1)  # Back to (batch, seq_len, features)\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(tcn_out)\n",
    "        \n",
    "        # We only need the output from the last time step of the encoder\n",
    "        last_step_out = transformer_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer to get the forecast\n",
    "        out = self.fc(last_step_out)\n",
    "        \n",
    "        # Reshape to (batch_size, horizon, num_quantiles)\n",
    "        out = out.view(-1, self.horizon, self.num_quantiles)\n",
    "        return out\n",
    "\n",
    "# Initialize model architecture if pipeline is valid\n",
    "if check_pipeline_state(\"Model definition\") and not featured_df.empty and train_indices is not None:\n",
    "    print(\"--- TCN+Transformer Hybrid Model Definition ---\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate feature dimensions\n",
    "        target_col_scaled = f\"{config.TARGET_COLUMN}_scaled\"\n",
    "        feature_cols = [c for c in featured_df.columns if c not in [config.TARGET_COLUMN, target_col_scaled]]\n",
    "        input_features = len(feature_cols)\n",
    "        \n",
    "        print(f\"Input features: {input_features}\")\n",
    "        print(f\"Feature columns: {feature_cols[:5]}{'...' if len(feature_cols) > 5 else ''}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model_hybrid = TCNTransformerHybrid(\n",
    "            input_size=input_features,\n",
    "            output_size=1,  # forecasting a single value per quantile\n",
    "            horizon=config.FORECAST_HORIZON,\n",
    "            num_quantiles=len(config.QUANTILES),\n",
    "            tcn_channels=[32, 64],\n",
    "            d_model=64,\n",
    "            nhead=4,\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ TCN+Transformer Hybrid model initialized successfully.\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model_hybrid.parameters()):,}\")\n",
    "        \n",
    "        # Store feature columns for later use\n",
    "        model_feature_cols = feature_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error initializing model: {str(e)}\")\n",
    "        model_hybrid = None\n",
    "        model_feature_cols = []\n",
    "else:\n",
    "    model_hybrid = None\n",
    "    model_feature_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b125c4-361f-423b-b31f-a96abd7791c6",
   "metadata": {},
   "source": [
    "# Training Utilities\n",
    "\n",
    "### üèãÔ∏è Training Time: Let's Get This Model Buff\n",
    "\n",
    "*It's the moment of truth! We have our brainy model and our clean, feature-rich data. Now it's time to put them together and start the training process. This is where the model learns the patterns we've been preparing it for.*\n",
    "\n",
    "**The Workout Plan**\n",
    "This block contains all the machinery for our model's training regimen. Here's the play-by-play:\n",
    "\n",
    "* **Creating Sequences:** First, we run our data through create_sequences. This function is like a cookie-cutter, chopping our long timeline into smaller, bite-sized \"look back at this, predict that\" samples that the model can actually learn from.\n",
    "\n",
    "* **The Loss Function (Pinball Loss):** How does a model know if it's right or wrong? It uses a loss function. Since we're predicting a range of possible outcomes (quantiles) and not just one number, we use a special loss function called pinball_loss. It cleverly scores our model on how accurate its entire predicted range is, not just its median guess.\n",
    "\n",
    "* **The Trainer (train_pytorch_model):** This is the head coach. It manages the whole workout, sending batches of data to the model, calculating the loss, and telling the model how to adjust its internal \"weights\" to get better. It runs this loop for several rounds (epochs).\n",
    "\n",
    "* **The Spotter (EarlyStopper):** We also have a spotter to prevent over-training. The EarlyStopper watches the model's performance on a separate validation dataset. If the model stops improving after a few epochs, the spotter calls it a day, saving us from wasting time and ending up with a less effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9421c0d-6d2a-4e29-bc6b-722324cb4722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def pinball_loss(y_pred, y_true, quantiles):\n",
    "    \"\"\"\n",
    "    Calculates the pinball loss for quantile regression.\n",
    "    y_pred shape: (batch_size, horizon, num_quantiles)\n",
    "    y_true shape: (batch_size, horizon)\n",
    "    \"\"\"\n",
    "    y_true = y_true.unsqueeze(-1)  # Add quantile dimension for broadcasting\n",
    "    error = y_true - y_pred\n",
    "    # Move quantiles to the correct device\n",
    "    q_tensor = torch.tensor(quantiles, device=y_pred.device).view(1, 1, -1)\n",
    "    loss = torch.max((q_tensor * error), ((q_tensor - 1) * error))\n",
    "    return loss.mean()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def create_sequences(data, lookback, horizon, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Creates sequences of features (X) and targets (y).\n",
    "    This version explicitly uses the provided feature_cols list.\n",
    "    \"\"\"\n",
    "    if data.empty or target_col not in data.columns:\n",
    "        return np.array([], dtype=np.float32).reshape(0, lookback, len(feature_cols)), np.array([], dtype=np.float32).reshape(0, horizon)\n",
    "    \n",
    "    # Check if all feature columns exist\n",
    "    missing_cols = [col for col in feature_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing feature columns: {missing_cols}\")\n",
    "        feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    X, y = [], []\n",
    "    # Ensure there's enough data to create at least one sequence\n",
    "    if len(data) < lookback + horizon:\n",
    "        return np.array([], dtype=np.float32).reshape(0, lookback, len(feature_cols)), np.array([], dtype=np.float32).reshape(0, horizon)\n",
    "        \n",
    "    for i in range(len(data) - lookback - horizon + 1):\n",
    "        # Select ONLY the specified feature columns for the input sequence\n",
    "        X.append(data[feature_cols].iloc[i:(i + lookback)].values)\n",
    "        y.append(data[target_col].iloc[(i + lookback):(i + lookback + horizon)].values)\n",
    "        \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "def train_pytorch_model(model, train_df, val_df, config, feature_cols):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with proper error handling and validation.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Model training\"):\n",
    "        return None, {}\n",
    "    \n",
    "    if model is None or train_df.empty:\n",
    "        pipeline_state.invalidate(\"Cannot train: model is None or training data is empty\")\n",
    "        return None, {}\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Training Pure PyTorch TCN+Transformer Model ---\")\n",
    "        device = torch.device(\"cuda\" if config.env_flags.get('CUDA_AVAILABLE', False) else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model.to(device)\n",
    "        target_col = f\"{config.TARGET_COLUMN}_scaled\"\n",
    "        \n",
    "        # Create training sequences\n",
    "        X_train, y_train = create_sequences(train_df, config.LOOKBACK_WINDOW, config.FORECAST_HORIZON, feature_cols, target_col)\n",
    "        \n",
    "        if X_train.shape[0] == 0:\n",
    "            pipeline_state.invalidate(\"No training sequences could be created\")\n",
    "            return None, {}\n",
    "        \n",
    "        print(f\"Training sequences: {X_train.shape[0]} samples\")\n",
    "        \n",
    "        # Create validation sequences\n",
    "        X_val, y_val = create_sequences(val_df, config.LOOKBACK_WINDOW, config.FORECAST_HORIZON, feature_cols, target_col)\n",
    "        \n",
    "        if X_val.shape[0] == 0:\n",
    "            print(\"Warning: No validation sequences available - training without validation\")\n",
    "            use_validation = False\n",
    "        else:\n",
    "            print(f\"Validation sequences: {X_val.shape[0]} samples\")\n",
    "            use_validation = True\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), \n",
    "            batch_size=config.BATCH_SIZE, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        if use_validation:\n",
    "            val_loader = DataLoader(\n",
    "                TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)), \n",
    "                batch_size=config.BATCH_SIZE\n",
    "            )\n",
    "        \n",
    "        # Setup training components\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "        early_stopper = EarlyStopper(patience=config.PATIENCE) if use_validation else None\n",
    "        \n",
    "        if use_validation:\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=config.PATIENCE // 2)\n",
    "        \n",
    "        scaler = torch.amp.GradScaler('cuda', enabled=config.USE_AMP) if torch.cuda.is_available() else torch.amp.GradScaler('cpu', enabled=False)\n",
    "        progress_bar = get_progress_bar()\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config.EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            for X_batch, y_batch in progress_bar(train_loader, desc=f\"Epoch {epoch+1}/{config.EPOCHS} [Train]\", leave=False):\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=config.USE_AMP):\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = pinball_loss(outputs, y_batch, config.QUANTILES)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = None\n",
    "            if use_validation:\n",
    "                model.eval()\n",
    "                val_loss_total = 0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in progress_bar(val_loader, desc=f\"Epoch {epoch+1}/{config.EPOCHS} [Val]\", leave=False):\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu', enabled=config.USE_AMP):\n",
    "                            outputs = model(X_batch)\n",
    "                            loss = pinball_loss(outputs, y_batch, config.QUANTILES)\n",
    "                        val_loss_total += loss.item()\n",
    "                \n",
    "                val_loss = val_loss_total / len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            \n",
    "            val_loss_str = f\"{val_loss:.4f}\" if val_loss is not None else \"N/A\"\n",
    "            print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss_str} | Time: {epoch_duration:.2f}s\")\n",
    "            \n",
    "            # Early stopping and learning rate scheduling\n",
    "            if use_validation and val_loss is not None:\n",
    "                if early_stopper and early_stopper(val_loss):\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "                scheduler.step(val_loss)\n",
    "        \n",
    "        pipeline_state.model_trained = True\n",
    "        print(\"Training complete.\")\n",
    "        \n",
    "        return model, {\n",
    "            \"train_losses\": train_losses, \n",
    "            \"val_losses\": val_losses if use_validation else [],\n",
    "            \"X_val\": X_val if use_validation else np.array([]),\n",
    "            \"y_val\": y_val if use_validation else np.array([])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error during training: {str(e)}\")\n",
    "        return None, {}\n",
    "\n",
    "print(\"Training utilities defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9e724-6d2d-47b3-ac6e-0e3322a7d8ef",
   "metadata": {},
   "source": [
    "# Model Training Execution\n",
    "### üí™ The Main Event: Kicking Off the Training\n",
    "\n",
    "*Alright, the warm-up is over. We've prepped the data, built the brain, and laid out the workout plan. This cell is the starting whistle.*\n",
    "\n",
    "* It hands our data and the model to the train_pytorch_model coach and kicks off the training loop. You'll see the progress for each round (epoch) as the model learns from the data and gets progressively smarter.\n",
    "\n",
    "* We don't even let the model catch its breath. As soon as the training is finished, we immediately have it make predictions on the validation data. This gives us our first, honest look at how well it actually learned its lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce3faf8-66d7-4b39-97a4-a1a9bc0f46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (720, 17)\n",
      "Validation data shape: (12, 17)\n",
      "\n",
      "--- Training Pure PyTorch TCN+Transformer Model ---\n",
      "Using device: cuda\n",
      "Training sequences: 637 samples\n",
      "Warning: No validation sequences available - training without validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2533330/1220099311.py:103: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10dc1a3ddca4d058158d6bdf7c28233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.5014 | Val Loss: N/A | Time: 3.62s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157560c3b2f2416daa64a0db406e3029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Train Loss: 0.3965 | Val Loss: N/A | Time: 0.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c89417beb431ea3cfb300089b6754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Train Loss: 0.3621 | Val Loss: N/A | Time: 0.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d02ae6c2c40443a90358be2a08232ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Train Loss: 0.3442 | Val Loss: N/A | Time: 0.08s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9762e0fc2474598defd937c34da69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Train Loss: 0.3314 | Val Loss: N/A | Time: 0.08s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ba1a2752014006ade917c6a0275a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Train Loss: 0.3210 | Val Loss: N/A | Time: 0.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edf968e917e49c7a54e4ee13ec949df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Train Loss: 0.3127 | Val Loss: N/A | Time: 0.08s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec29623907345ac8c66bb8b07751fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Train Loss: 0.3053 | Val Loss: N/A | Time: 0.08s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7c111fa4914e719476a0bad0c81b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Train Loss: 0.2997 | Val Loss: N/A | Time: 0.09s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd32b40e413d495093835e4491b2d182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.2929 | Val Loss: N/A | Time: 0.09s\n",
      "Training complete.\n",
      "Model training completed successfully.\n",
      "No validation data available for predictions.\n"
     ]
    }
   ],
   "source": [
    "# Execute model training\n",
    "if check_pipeline_state(\"Training execution\") and model_hybrid is not None and train_indices is not None:\n",
    "    try:\n",
    "        # Prepare training and validation data\n",
    "        train_data = featured_df.iloc[train_indices]\n",
    "        val_data = featured_df.iloc[test_indices]\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Validation data shape: {val_data.shape}\")\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, training_history = train_pytorch_model(\n",
    "            model_hybrid, \n",
    "            train_data, \n",
    "            val_data, \n",
    "            config, \n",
    "            model_feature_cols\n",
    "        )\n",
    "        \n",
    "        if trained_model is not None and pipeline_state.model_trained:\n",
    "            print(\"Model training completed successfully.\")\n",
    "            \n",
    "            # Store training artifacts\n",
    "            X_val = training_history.get('X_val', np.array([]))\n",
    "            y_val = training_history.get('y_val', np.array([]))\n",
    "            \n",
    "            if X_val.shape[0] > 0:\n",
    "                print(f\"Validation sequences available: {X_val.shape}\")\n",
    "                \n",
    "                # Generate predictions\n",
    "                device = torch.device(\"cuda\" if config.env_flags.get('CUDA_AVAILABLE', False) else \"cpu\")\n",
    "                trained_model.to(device)\n",
    "                trained_model.eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    X_val_tensor = torch.from_numpy(X_val).to(device)\n",
    "                    y_pred_quantiles = trained_model(X_val_tensor).cpu().numpy()\n",
    "                \n",
    "                pipeline_state.predictions_generated = True\n",
    "                print(f\"Predictions generated: {y_pred_quantiles.shape}\")\n",
    "            else:\n",
    "                print(\"No validation data available for predictions.\")\n",
    "                y_pred_quantiles = np.array([])\n",
    "        else:\n",
    "            pipeline_state.invalidate(\"Model training failed\")\n",
    "            trained_model = None\n",
    "            X_val, y_val, y_pred_quantiles = np.array([]), np.array([]), np.array([])\n",
    "            \n",
    "    except Exception as e:\n",
    "        pipeline_state.invalidate(f\"Error during training execution: {str(e)}\")\n",
    "        trained_model = None\n",
    "        X_val, y_val, y_pred_quantiles = np.array([]), np.array([]), np.array([])\n",
    "else:\n",
    "    print(\"Skipping model training - prerequisites not met.\")\n",
    "    trained_model = None\n",
    "    X_val, y_val, y_pred_quantiles = np.array([]), np.array([]), np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea4289-0fce-4833-8553-83c394856f21",
   "metadata": {},
   "source": [
    "# Evaluation & Metrics\n",
    "### üèÜ The Report Card: Did We Pass?\n",
    "\n",
    "Training is done and our model has made its first predictions. But are they any good? It's time to grade our model's performance.\n",
    "\n",
    "This final block is the judge and jury. It takes the model's predictions and compares them to the actual, real-world answers. Before it can deliver a fair verdict, it has to reverse the scaling we applied way back at the beginning, translating the model's scaled-down numbers back into their original, real-world units.\n",
    "\n",
    "**The Final Scores**\n",
    "We're using a few key metrics to get a well-rounded view of our model's performance:\n",
    "\n",
    "* MAE & RMSE: These are the classics. They tell us, on average, how far off our model's main prediction was from the actual value. Lower is better.\n",
    "\n",
    "* MAPE: This metric gives us the error as a percentage, which is often easier to understand. A MAPE of 10% means we were off by 10% on average.\n",
    "\n",
    "* Pinball Loss: This is the official score for our quantile predictions. It grades the entire predicted range, not just the single best guess.\n",
    "\n",
    "* Coverage: This checks if the real answer fell within our predicted range (between the P10 and P90 quantiles). A good model should have its coverage close to the 80% range it was aiming for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeda2369-a926-48a6-b30d-dee2404cc12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping evaluation - no valid predictions available\n",
      "Evaluation metrics functions defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    if not np.any(mask):\n",
    "        return np.inf\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def coverage(y_true, y_lower, y_upper):\n",
    "    return np.mean((y_true >= y_lower) & (y_true <= y_upper)) * 100\n",
    "\n",
    "def pinball_loss_metric(y_true, y_pred_quantiles, quantiles):\n",
    "    \"\"\"Numpy version of the pinball loss for evaluation.\"\"\"\n",
    "    if y_pred_quantiles.size == 0 or y_true.size == 0:\n",
    "        return np.inf\n",
    "        \n",
    "    loss = 0\n",
    "    y_true_exp = np.expand_dims(y_true, axis=-1)\n",
    "    error = y_true_exp - y_pred_quantiles\n",
    "    for i, q in enumerate(quantiles):\n",
    "        loss += np.mean(np.maximum(q * error[..., i], (q - 1) * error[..., i]))\n",
    "    return loss / len(quantiles)\n",
    "\n",
    "def evaluate_forecast(y_true, y_pred_quantiles, quantiles, scaler, target_col):\n",
    "    \"\"\"\n",
    "    Computes a dictionary of evaluation metrics.\n",
    "    Assumes y_pred_quantiles and y_true are scaled.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Forecast evaluation\"):\n",
    "        return {}\n",
    "    \n",
    "    if y_pred_quantiles.size == 0 or y_true.size == 0:\n",
    "        print(\"Warning: Empty predictions or targets, cannot evaluate\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Find quantile indices\n",
    "        p10_idx = quantiles.index(0.1) if 0.1 in quantiles else 0\n",
    "        p50_idx = quantiles.index(0.5) if 0.5 in quantiles else len(quantiles)//2\n",
    "        p90_idx = quantiles.index(0.9) if 0.9 in quantiles else -1\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        if isinstance(scaler, dict):  # Manual scaler\n",
    "            mean, std = scaler['mean'], scaler['std']\n",
    "            y_true_orig = y_true * std + mean\n",
    "            y_pred_quantiles_orig = y_pred_quantiles * std + mean\n",
    "        else:  # Scikit-learn scaler\n",
    "            y_true_orig = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Reshape predictions for inverse transform\n",
    "            num_samples, horizon, num_quantiles = y_pred_quantiles.shape\n",
    "            preds_flat = y_pred_quantiles.reshape(-1, num_quantiles)\n",
    "            preds_orig_flat = np.zeros_like(preds_flat)\n",
    "            for i in range(num_quantiles):\n",
    "                preds_orig_flat[:, i] = scaler.inverse_transform(preds_flat[:, i].reshape(-1, 1)).flatten()\n",
    "            y_pred_quantiles_orig = preds_orig_flat.reshape(num_samples, horizon, num_quantiles)\n",
    "        \n",
    "        y_pred_p50_orig = y_pred_quantiles_orig[..., p50_idx].flatten()\n",
    "        y_true_flat_orig = y_true_orig.flatten()\n",
    "        \n",
    "        metrics = {\n",
    "            \"MAE\": mae(y_true_flat_orig, y_pred_p50_orig),\n",
    "            \"RMSE\": rmse(y_true_flat_orig, y_pred_p50_orig),\n",
    "            \"MAPE\": mape(y_true_flat_orig, y_pred_p50_orig),\n",
    "            \"Pinball Loss\": pinball_loss_metric(y_true_orig, y_pred_quantiles_orig, quantiles),\n",
    "        }\n",
    "        \n",
    "        # Add coverage if we have the right quantiles\n",
    "        if 0.1 in quantiles and 0.9 in quantiles:\n",
    "            metrics[\"Coverage (P10-P90)\"] = coverage(\n",
    "                y_true_orig, \n",
    "                y_pred_quantiles_orig[..., p10_idx], \n",
    "                y_pred_quantiles_orig[..., p90_idx]\n",
    "            )\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Execute evaluation if we have valid predictions\n",
    "if (check_pipeline_state(\"Evaluation execution\") and \n",
    "    pipeline_state.predictions_generated and \n",
    "    'y_pred_quantiles' in locals() and \n",
    "    y_pred_quantiles.size > 0 and \n",
    "    'y_val' in locals() and \n",
    "    y_val.size > 0):\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Evaluation Metrics ---\")\n",
    "        \n",
    "        metrics = evaluate_forecast(y_val, y_pred_quantiles, config.QUANTILES, scaler, config.TARGET_COLUMN)\n",
    "        \n",
    "        if metrics:\n",
    "            for key, value in metrics.items():\n",
    "                if np.isfinite(value):\n",
    "                    print(f\"{key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{key}: Invalid (inf/nan)\")\n",
    "        else:\n",
    "            print(\"No valid metrics could be calculated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation execution: {str(e)}\")\n",
    "        metrics = {}\n",
    "else:\n",
    "    print(\"Skipping evaluation - no valid predictions available\")\n",
    "    metrics = {}\n",
    "\n",
    "print(\"Evaluation metrics functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed89728-6b43-4095-b238-b283fec8fa1f",
   "metadata": {},
   "source": [
    "# What-If Scenario Engine\n",
    "\n",
    "### üîÆ The Crystal Ball: Playing with the Future\n",
    "\n",
    "*A forecast is just a starting point. The real fun begins when we ask, \"What if...?\"*\n",
    "\n",
    "This final block is our what-if engine, a playground for stress-testing our forecast. We're about to throw a wrench in the works to see how our calm, predictable energy grid handles a sudden, massive change.\n",
    "\n",
    "**Here's the Game Plan:**\n",
    "* **Unleash the EV Tsunami:** First, we simulate a sudden rush of thousands of Electric Vehicles all plugging in after work. The generate_ev_load_profile function creates this tidal wave of new energy demand. You are in control of the chaos, using the UI sliders to decide just how many EVs show up to the party.\n",
    "\n",
    "* **The Aftermath:** We then smash this new wave of demand onto our original, \"business-as-usual\" forecast. This shows us the new, combined reality.\n",
    "\n",
    "The result is a clear picture and a summary of our little experiment, showing exactly how much stress this EV-pocalypse puts on the grid and what our new, much higher, peak demand looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909f13c8-a014-46d2-acf3-9f6bc94d7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping What-If Scenario Engine - prerequisites not met\n",
      "What-If Scenario Engine completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_ev_load_profile(\n",
    "    num_evs=1000, \n",
    "    charging_power_kw=7.2, \n",
    "    charging_duration_h=5, \n",
    "    start_time_mean=18, \n",
    "    start_time_std=2, \n",
    "    horizon_h=24,\n",
    "    baseline_index=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a synthetic EV charging load profile for a 24-hour period.\n",
    "    Each EV starts charging at a normally distributed start hour,\n",
    "    charges at charging_power_kw for charging_duration_h hours.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"EV load profile generation\"):\n",
    "        return pd.Series()\n",
    "        \n",
    "    try:\n",
    "        print(f\"\\n--- Generating EV scenario for {num_evs} EVs ---\")\n",
    "        total_load = np.zeros(horizon_h)\n",
    "        start_times = np.random.normal(start_time_mean, start_time_std, num_evs)\n",
    "        \n",
    "        for start_hour in start_times:\n",
    "            start_idx = int(round(start_hour)) % horizon_h\n",
    "            end_idx = start_idx + int(round(charging_duration_h))\n",
    "            # Add load for the charging duration, wrap around if needed\n",
    "            for i in range(start_idx, end_idx):\n",
    "                total_load[i % horizon_h] += charging_power_kw\n",
    "        \n",
    "        # Build index\n",
    "        if baseline_index is not None:\n",
    "            index = baseline_index[:horizon_h]\n",
    "        else:\n",
    "            index = pd.date_range('2024-01-01 00:00:00', periods=horizon_h, freq='H')\n",
    "                \n",
    "        ev_load_profile = pd.Series(total_load, index=index, name=\"ev_load_kw\")\n",
    "        print(f\"Total EV energy generated: {total_load.sum():.1f} kWh \"\n",
    "              f\"({num_evs} EVs √ó {charging_duration_h}h √ó {charging_power_kw} kW avg)\")\n",
    "        return ev_load_profile\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating EV load profile: {str(e)}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def create_scenario_dataframe(baseline_forecast, baseline_index, ev_profile):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive scenario dataframe with baseline and EV loads.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        scenario_df = pd.DataFrame({'baseline_load': baseline_forecast}, index=baseline_index)\n",
    "        \n",
    "        if len(ev_profile) > 0:\n",
    "            aligned_ev_load = pd.Series(0.0, index=scenario_df.index)\n",
    "            min_length = min(len(scenario_df), len(ev_profile))\n",
    "            aligned_ev_load.iloc[:min_length] = ev_profile.iloc[:min_length].values\n",
    "            scenario_df['ev_load'] = aligned_ev_load\n",
    "        else:\n",
    "            scenario_df['ev_load'] = 0.0\n",
    "            \n",
    "        scenario_df['total_load_with_ev'] = scenario_df['baseline_load'] + scenario_df['ev_load']\n",
    "        \n",
    "        return scenario_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating scenario dataframe: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# === Execute scenario generation ===\n",
    "if (check_pipeline_state(\"Scenario generation\") and \n",
    "    pipeline_state.predictions_generated and \n",
    "    'y_pred_quantiles' in locals() and \n",
    "    y_pred_quantiles.size > 0 and \n",
    "    'test_indices' in locals() and \n",
    "    test_indices is not None):\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- What-If Scenario Analysis ---\")\n",
    "        \n",
    "        # Baseline forecast from predictions\n",
    "        p50_idx = config.QUANTILES.index(0.5) if 0.5 in config.QUANTILES else len(config.QUANTILES)//2\n",
    "        baseline_forecast_scaled = y_pred_quantiles[0, :, p50_idx]\n",
    "        \n",
    "        if isinstance(scaler, dict):\n",
    "            baseline_forecast = baseline_forecast_scaled * scaler['std'] + scaler['mean']\n",
    "        else:\n",
    "            baseline_forecast = scaler.inverse_transform(baseline_forecast_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Forecast index\n",
    "        val_data = featured_df.iloc[test_indices]\n",
    "        forecast_start_idx = config.LOOKBACK_WINDOW\n",
    "        forecast_end_idx = forecast_start_idx + config.FORECAST_HORIZON\n",
    "        \n",
    "        if len(val_data) >= forecast_end_idx:\n",
    "            forecast_index = val_data.index[forecast_start_idx:forecast_end_idx]\n",
    "        else:\n",
    "            start_time = val_data.index[0] if len(val_data) > 0 else pd.Timestamp('2024-01-01')\n",
    "            forecast_index = pd.date_range(start_time, periods=config.FORECAST_HORIZON, freq='H')\n",
    "        \n",
    "        print(f\"Baseline forecast range: {forecast_index[0]} ‚Üí {forecast_index[-1]}\")\n",
    "        \n",
    "        # === üîë Generate EV load profile using UI sliders ===\n",
    "        ev_load = generate_ev_load_profile(\n",
    "            num_evs=num_evs_slider.value,\n",
    "            charging_power_kw=charging_power_slider.value,\n",
    "            charging_duration_h=charging_duration_slider.value,\n",
    "            horizon_h=config.FORECAST_HORIZON,\n",
    "            baseline_index=forecast_index\n",
    "        )\n",
    "        \n",
    "        # Create scenario dataframe\n",
    "        scenario_df = create_scenario_dataframe(baseline_forecast, forecast_index, ev_load)\n",
    "        \n",
    "        if not scenario_df.empty:\n",
    "            print(f\"‚úÖ Scenario created successfully with {len(scenario_df)} time points\")\n",
    "            print(\"\\nScenario Summary:\")\n",
    "            print(f\"  Baseline peak load: {scenario_df['baseline_load'].max():.1f} kW\")\n",
    "            print(f\"  Total EV load: {scenario_df['ev_load'].sum():.1f} kWh\")\n",
    "            print(f\"  Peak load with EVs: {scenario_df['total_load_with_ev'].max():.1f} kW\")\n",
    "            print(f\"  Peak increase: {((scenario_df['total_load_with_ev'].max() / scenario_df['baseline_load'].max() - 1) * 100):.1f}%\")\n",
    "            \n",
    "            if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                scenario_df[['baseline_load', 'total_load_with_ev']].plot()\n",
    "                plt.title(\"What-If Scenario: Baseline Load vs. Load with Uncontrolled EV Charging\")\n",
    "                plt.ylabel(\"Load (kW)\")\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.legend(['Baseline Load', 'Load with EVs'])\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"‚ùå Failed to create scenario dataframe\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scenario generation: {str(e)}\")\n",
    "        scenario_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Skipping What-If Scenario Engine - prerequisites not met\")\n",
    "    scenario_df = pd.DataFrame()\n",
    "\n",
    "print(\"What-If Scenario Engine completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec23c5c-eae0-4339-a968-cd2538995792",
   "metadata": {},
   "source": [
    "# Load Shifting Optimizer\n",
    "### üß† Smart Charging: Playing Tetris with Energy\n",
    "\n",
    "*Okay, our last \"what-if\" scenario created a huge, chaotic spike in energy demand. That's bad for the grid and expensive for everyone. But what if we could be smarter about it? What if we could tell all those EVs to charge when energy is cheapest and most plentiful?*\n",
    "\n",
    "This block is our smart-charging optimizer. Think of it as playing a game of Tetris with the EV charging load. It takes that big, awkward block of energy demand and finds the perfect empty spots to drop it into, flattening the overall demand curve.\n",
    "\n",
    "**The Strategy: A Greedy Approach**\n",
    "*Our optimizer, greedy_load_shifter, uses a simple but powerful \"greedy\" strategy:*\n",
    "\n",
    "* **Identify the Quiet Hours:** First, it looks for the off-peak window (e.g., between 10 PM and 7 AM) when the baseline energy demand is lowest.\n",
    "\n",
    "* **Find the Best Spots:** Within that window, it finds the absolute quietest hours‚Äîthe bottom of the \"valley\" in our energy usage graph.\n",
    "\n",
    "* **Shift the Load:** It then takes the total energy needed by the EVs and strategically pours it into those quietest hours, starting with the very lowest and filling it up.\n",
    "\n",
    "The result is a much smoother, flatter energy profile. Instead of a scary, sharp peak, the demand is spread out, which is healthier for the grid and cheaper for consumers. The final plot shows the chaotic \"uncontrolled\" spike versus our new, much calmer \"optimized\" load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c9cf17c-f51c-465d-9896-7dcbd63ecc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Load Shifting Optimizer - no valid scenario available\n",
      "Load Shifting Optimizer completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def greedy_load_shifter(baseline_load, flexible_load, charging_window=(22, 7), charging_duration_h=None):\n",
    "    \"\"\"\n",
    "    Greedy optimizer:\n",
    "      - Shifts flexible EV load into lowest-load off-peak hours.\n",
    "      - Respects user-defined charging window and charging duration.\n",
    "    Returns optimized total load and shifted flexible load.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Load shifting optimization\"):\n",
    "        return pd.Series(), pd.Series()\n",
    "        \n",
    "    if baseline_load.empty or flexible_load.empty:\n",
    "        print(\"‚ö†Ô∏è Warning: Empty load profiles provided to optimizer\")\n",
    "        return baseline_load.copy(), flexible_load.copy()\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Running Greedy Load Shifting Optimizer ---\")\n",
    "        \n",
    "        optimized_flexible_load = pd.Series(0.0, index=flexible_load.index)\n",
    "        total_energy_to_shift = flexible_load.sum()  # Assuming hourly data, sum ~ kWh\n",
    "        \n",
    "        print(f\"Total EV energy to shift: {total_energy_to_shift:.1f} kWh\")\n",
    "        \n",
    "        # Identify valid charging slots\n",
    "        start, end = charging_window\n",
    "        if start > end:  # Overnight window like 22 ‚Üí 7\n",
    "            valid_hours = (baseline_load.index.hour >= start) | (baseline_load.index.hour < end)\n",
    "        else:  # Daytime window\n",
    "            valid_hours = (baseline_load.index.hour >= start) & (baseline_load.index.hour < end)\n",
    "        \n",
    "        available_slots = baseline_load[valid_hours]\n",
    "        print(f\"Available off-peak slots: {len(available_slots)} hours\")\n",
    "        \n",
    "        if len(available_slots) > 0:\n",
    "            # Sort slots by baseline load (lowest first)\n",
    "            available_slots_sorted = available_slots.sort_values()\n",
    "            \n",
    "            # If user specified duration, restrict number of slots\n",
    "            if charging_duration_h is not None:\n",
    "                slots_needed = min(int(charging_duration_h), len(available_slots_sorted))\n",
    "                selected_slots = available_slots_sorted.index[:slots_needed]\n",
    "            else:\n",
    "                selected_slots = available_slots_sorted.index  # use all\n",
    "            \n",
    "            print(f\"Using {len(selected_slots)} slots for charging\")\n",
    "            \n",
    "            # True greedy allocation: fill lowest slots until energy is assigned\n",
    "            total_energy_remaining = total_energy_to_shift\n",
    "            energy_per_slot = total_energy_to_shift / len(selected_slots)\n",
    "            \n",
    "            for ts in selected_slots:\n",
    "                if total_energy_remaining <= 0:\n",
    "                    break\n",
    "                allocation = min(energy_per_slot, total_energy_remaining)\n",
    "                optimized_flexible_load[ts] = allocation\n",
    "                total_energy_remaining -= allocation\n",
    "            \n",
    "            if total_energy_remaining > 0:\n",
    "                print(f\"‚ö†Ô∏è {total_energy_remaining:.1f} kWh could not be shifted (not enough slots)\")\n",
    "            \n",
    "            print(f\"‚úÖ Energy distributed across {len(selected_slots)} slots\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No available off-peak slots to shift load into. Load remains unchanged.\")\n",
    "            optimized_flexible_load = flexible_load.copy()\n",
    "\n",
    "        optimized_total_load = baseline_load + optimized_flexible_load\n",
    "\n",
    "        # Metrics\n",
    "        original_peak = (baseline_load + flexible_load).max()\n",
    "        optimized_peak = optimized_total_load.max()\n",
    "        peak_reduction = ((original_peak - optimized_peak) / original_peak * 100) if original_peak > 0 else 0\n",
    "        \n",
    "        original_lf = (baseline_load + flexible_load).mean() / original_peak if original_peak > 0 else 0\n",
    "        optimized_lf = optimized_total_load.mean() / optimized_peak if optimized_peak > 0 else 0\n",
    "        lf_improvement = (optimized_lf - original_lf) * 100\n",
    "        \n",
    "        print(\"Optimization Results:\")\n",
    "        print(f\"  - Original Peak Load: {original_peak:,.2f} kW\")\n",
    "        print(f\"  - Optimized Peak Load: {optimized_peak:,.2f} kW\")\n",
    "        print(f\"  - Peak Reduction: {peak_reduction:.2f}%\")\n",
    "        print(f\"  - Load Factor Improvement: {lf_improvement:.2f} percentage points\")\n",
    "\n",
    "        return optimized_total_load, optimized_flexible_load\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during load shifting optimization: {str(e)}\")\n",
    "        return baseline_load.copy(), flexible_load.copy()\n",
    "\n",
    "# Execute optimization if scenario is available\n",
    "if (check_pipeline_state(\"Optimization execution\") and \n",
    "    'scenario_data' in locals() and \n",
    "    not scenario_data.empty and \n",
    "    'baseline_load' in scenario_data.columns and \n",
    "    'ev_load' in scenario_data.columns):\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n--- Load Shifting Optimization ---\")\n",
    "        \n",
    "        # Pass charging_duration_h from UI if available\n",
    "        charging_duration_h = charging_duration_slider.value if 'charging_duration_slider' in locals() else None\n",
    "        \n",
    "        optimized_load, shifted_ev_load = greedy_load_shifter(\n",
    "            scenario_data['baseline_load'], \n",
    "            scenario_data['ev_load'],\n",
    "            charging_window=(22, 7),\n",
    "            charging_duration_h=charging_duration_h\n",
    "        )\n",
    "        \n",
    "        if not optimized_load.empty and not shifted_ev_load.empty:\n",
    "            scenario_data['optimized_total_load'] = optimized_load\n",
    "            scenario_data['shifted_ev_load'] = shifted_ev_load\n",
    "            \n",
    "            if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                scenario_data[['total_load_with_ev', 'optimized_total_load']].plot(style=['--', '-'])\n",
    "                scenario_data['baseline_load'].plot(style=':', color='gray')\n",
    "                plt.title(\"Optimization Result: Uncontrolled vs. Optimized Charging\")\n",
    "                plt.ylabel(\"Load (kW)\")\n",
    "                plt.legend(['Uncontrolled Total Load', 'Optimized Total Load', 'Baseline Load'])\n",
    "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            print(\"‚úÖ Optimization completed successfully.\")\n",
    "        else:\n",
    "            print(\"‚ùå Optimization failed to produce valid results.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during optimization execution: {str(e)}\")\n",
    "else:\n",
    "    print(\"Skipping Load Shifting Optimizer - no valid scenario available\")\n",
    "\n",
    "print(\"Load Shifting Optimizer completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32775cbc-edca-4990-bdfe-58b3fa663169",
   "metadata": {},
   "source": [
    "# Advanced Reporting Dashboard\n",
    "### üìä The Final Verdict: The Executive Dashboard\n",
    "\n",
    "*We've run our simulation and optimized the chaos. Now it's time for the final reveal. This block is the grand finale, where we crunch all the numbers and present them in a clear, comprehensive dashboard. Think of it as the executive summary that tells us whether our smart-charging strategy was a genius move or a waste of time.*\n",
    "\n",
    "**What's on the Report?** \\\n",
    "**The generate_advanced_report function is our in-house data artist and accountant. It takes all our scenario data and calculates the most important metrics to answer two key questions: \"How much stress did we take off the grid?\" and \"How much money did we save?\"**\n",
    "\n",
    "* **Grid Impact Analysis:** This section focuses on the health of the power grid. We're looking at the Peak Reduction (how much we squashed that scary spike) and the Load Factor Improvement. A better load factor means we're using our power infrastructure more efficiently, which is a big win for everyone.\n",
    "\n",
    "* **Economic Impact:** The calculate_cost_savings function plays accountant, applying different prices for peak (expensive) and off-peak (cheap) hours. It calculates the cost of the chaotic, uncontrolled charging versus our smart, optimized charging, and presents the final cost savings as a simple percentage.\n",
    "\n",
    "The end result is a beautiful dashboard with charts and a summary report, giving us a complete, at-a-glance understanding of just how effective our optimization strategy really was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc1f220a-0c10-424b-8536-14316fe40e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Advanced Reporting - no valid scenario available\n",
      "Advanced Reporting Dashboard completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_cost_savings(scenario_df, peak_price=0.30, off_peak_price=0.10, peak_hours=(16, 21)):\n",
    "    \"\"\"\n",
    "    Calculates cost savings from load shifting based on time-of-use pricing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'ev_load' not in scenario_df.columns or 'shifted_ev_load' not in scenario_df.columns:\n",
    "            return 0, 0, 0\n",
    "            \n",
    "        # Create price series\n",
    "        price_series = pd.Series(index=scenario_df.index, dtype=float)\n",
    "        start_hour, end_hour = peak_hours\n",
    "        \n",
    "        for timestamp in scenario_df.index:\n",
    "            hour = timestamp.hour\n",
    "            if start_hour <= hour <= end_hour:\n",
    "                price_series[timestamp] = peak_price\n",
    "            else:\n",
    "                price_series[timestamp] = off_peak_price\n",
    "        \n",
    "        # Calculate costs\n",
    "        cost_before = (scenario_df['ev_load'] * price_series).sum()\n",
    "        cost_after = (scenario_df['shifted_ev_load'] * price_series).sum()\n",
    "        cost_savings = cost_before - cost_after\n",
    "        cost_savings_percent = (cost_savings / cost_before * 100) if cost_before > 0 else 0\n",
    "        \n",
    "        return cost_before, cost_after, cost_savings_percent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cost savings: {str(e)}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "def generate_advanced_report(scenario_df):\n",
    "    \"\"\"\n",
    "    Calculates advanced grid metrics and generates a comprehensive dashboard.\n",
    "    \"\"\"\n",
    "    if not check_pipeline_state(\"Advanced reporting\"):\n",
    "        return\n",
    "        \n",
    "    if scenario_df.empty:\n",
    "        print(\"Cannot generate report because scenario_df is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- Generating Advanced Report ---\")\n",
    "        \n",
    "        # Extract load profiles\n",
    "        uncontrolled_load = scenario_df['total_load_with_ev']\n",
    "        baseline_load = scenario_df['baseline_load']\n",
    "        \n",
    "        # Check if optimization was performed\n",
    "        if 'optimized_total_load' in scenario_df.columns:\n",
    "            optimized_load = scenario_df['optimized_total_load']\n",
    "            optimization_performed = True\n",
    "        else:\n",
    "            optimized_load = uncontrolled_load.copy()\n",
    "            optimization_performed = False\n",
    "            \n",
    "        # Calculate key metrics\n",
    "        total_energy_shifted_mwh = scenario_df['ev_load'].sum() / 1000  # Convert kW to MWh\n",
    "        \n",
    "        # Load Factor calculations\n",
    "        lf_baseline = baseline_load.mean() / baseline_load.max() if baseline_load.max() > 0 else 0\n",
    "        lf_uncontrolled = uncontrolled_load.mean() / uncontrolled_load.max() if uncontrolled_load.max() > 0 else 0\n",
    "        lf_optimized = optimized_load.mean() / optimized_load.max() if optimized_load.max() > 0 else 0\n",
    "        lf_improvement = (lf_optimized - lf_uncontrolled) * 100\n",
    "        \n",
    "        # Peak reduction\n",
    "        peak_reduction_percent = ((uncontrolled_load.max() - optimized_load.max()) / uncontrolled_load.max() * 100) if uncontrolled_load.max() > 0 else 0\n",
    "        \n",
    "        # Cost analysis\n",
    "        cost_before, cost_after, cost_savings_percent = calculate_cost_savings(scenario_df)\n",
    "        \n",
    "        # Print summary metrics\n",
    "        print(\"Grid Impact Analysis:\")\n",
    "        print(f\"  Total EV Energy: {total_energy_shifted_mwh:.2f} MWh\")\n",
    "        print(f\"  Baseline Peak: {baseline_load.max():.1f} kW\")\n",
    "        print(f\"  Uncontrolled Peak: {uncontrolled_load.max():.1f} kW\")\n",
    "        print(f\"  Optimized Peak: {optimized_load.max():.1f} kW\")\n",
    "        print(f\"  Peak Reduction: {peak_reduction_percent:.1f}%\")\n",
    "        print(f\"  Load Factor Improvement: {lf_improvement:.2f} percentage points\")\n",
    "        print(f\"  Cost Savings: {cost_savings_percent:.1f}%\")\n",
    "        \n",
    "        # Generate visualization dashboard if matplotlib is available\n",
    "        if config.env_flags.get('USE_MATPLOTLIB', False):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            fig.suptitle('Smart Grid Optimization Dashboard', fontsize=20, fontweight='bold')\n",
    "            \n",
    "            # Plot 1: Load Profiles (Time Series)\n",
    "            ax = axs[0, 0]\n",
    "            baseline_load.plot(ax=ax, style=':', color='gray', label='Baseline (no EVs)', alpha=0.8)\n",
    "            uncontrolled_load.plot(ax=ax, style='--', color='red', label='With Uncontrolled EVs', alpha=0.8)\n",
    "            if optimization_performed:\n",
    "                optimized_load.plot(ax=ax, style='-', color='green', label='With Optimized EVs', alpha=0.9, linewidth=2)\n",
    "            ax.set_title('Load Profile Comparison', fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel('Electricity Demand (kW)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Plot 2: Load Distribution (Histogram)\n",
    "            ax = axs[0, 1]\n",
    "            uncontrolled_load.plot(kind='hist', bins=20, ax=ax, alpha=0.7, color='red', label='Uncontrolled')\n",
    "            if optimization_performed:\n",
    "                optimized_load.plot(kind='hist', bins=20, ax=ax, alpha=0.7, color='green', label='Optimized')\n",
    "            ax.axvline(uncontrolled_load.max(), color='red', linestyle='--', alpha=0.8, \n",
    "                      label=f'Uncontrolled Peak: {uncontrolled_load.max():.0f} kW')\n",
    "            if optimization_performed:\n",
    "                ax.axvline(optimized_load.max(), color='green', linestyle='--', alpha=0.8, \n",
    "                          label=f'Optimized Peak: {optimized_load.max():.0f} kW')\n",
    "            ax.set_title('Load Distribution', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Electricity Demand (kW)')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.legend()\n",
    "            \n",
    "            # Plot 3: EV Charging Patterns\n",
    "            ax = axs[1, 0]\n",
    "            if 'ev_load' in scenario_df.columns:\n",
    "                scenario_df['ev_load'].plot(ax=ax, style='--', color='orange', label='Original EV Load', alpha=0.8)\n",
    "            if 'shifted_ev_load' in scenario_df.columns and optimization_performed:\n",
    "                scenario_df['shifted_ev_load'].plot(ax=ax, style='-', color='blue', label='Shifted EV Load', linewidth=2)\n",
    "            ax.set_title('EV Charging Patterns', fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel('EV Load (kW)')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Plot 4: Summary Metrics\n",
    "            ax = axs[1, 1]\n",
    "            ax.axis('off')\n",
    "            \n",
    "            summary_text = (\n",
    "                \"OPTIMIZATION SUMMARY\\n\"\n",
    "                \"=\" * 40 + \"\\n\\n\"\n",
    "                f\"Total EV Energy: {total_energy_shifted_mwh:.2f} MWh\\n\\n\"\n",
    "                f\"Peak Load Reduction:\\n\"\n",
    "                f\"  {uncontrolled_load.max():,.0f} ‚Üí {optimized_load.max():,.0f} kW\\n\"\n",
    "                f\"  Reduction: {peak_reduction_percent:.1f}%\\n\\n\"\n",
    "                f\"Load Factor:\\n\"\n",
    "                f\"  Baseline: {lf_baseline:.3f}\\n\"\n",
    "                f\"  Uncontrolled: {lf_uncontrolled:.3f}\\n\"\n",
    "                f\"  Optimized: {lf_optimized:.3f}\\n\"\n",
    "                f\"  Improvement: +{lf_improvement:.1f} pts\\n\\n\"\n",
    "                f\"Economic Impact:\\n\"\n",
    "                f\"  Original Cost: ${cost_before:.2f}\\n\"\n",
    "                f\"  Optimized Cost: ${cost_after:.2f}\\n\"\n",
    "                f\"  Savings: {cost_savings_percent:.1f}%\\n\\n\"\n",
    "                f\"Grid Stress Reduction:\\n\"\n",
    "                f\"  Peak-to-Average Ratio: {optimized_load.max()/optimized_load.mean():.2f}\\n\"\n",
    "                f\"  Load Variability: {optimized_load.std()/optimized_load.mean():.3f}\"\n",
    "            )\n",
    "            \n",
    "            ax.text(0.05, 0.95, summary_text, ha='left', va='top', fontsize=11, \n",
    "                   fontfamily='monospace', transform=ax.transAxes,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.94)  # Make room for main title\n",
    "            plt.show()\n",
    "        \n",
    "        return {\n",
    "            'total_energy_mwh': total_energy_shifted_mwh,\n",
    "            'peak_reduction_percent': peak_reduction_percent,\n",
    "            'load_factor_improvement': lf_improvement,\n",
    "            'cost_savings_percent': cost_savings_percent,\n",
    "            'baseline_peak_kw': baseline_load.max(),\n",
    "            'uncontrolled_peak_kw': uncontrolled_load.max(),\n",
    "            'optimized_peak_kw': optimized_load.max()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating advanced report: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Generate advanced report if scenario is available\n",
    "if (check_pipeline_state(\"Advanced report generation\") and \n",
    "    'scenario_df' in locals() and \n",
    "    not scenario_df.empty):\n",
    "    \n",
    "    try:\n",
    "        report_metrics = generate_advanced_report(scenario_df)\n",
    "        \n",
    "        if report_metrics:\n",
    "            print(\"\\nAdvanced report generated successfully.\")\n",
    "        else:\n",
    "            print(\"Advanced report generation failed.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during advanced report generation: {str(e)}\")\n",
    "else:\n",
    "    print(\"Skipping Advanced Reporting - no valid scenario available\")\n",
    "    report_metrics = {}\n",
    "\n",
    "print(\"Advanced Reporting Dashboard completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55448cb-17f6-40c0-b268-aaaf4701cb7a",
   "metadata": {},
   "source": [
    "# ü©π The Duct Tape Fix: A Bigger Validation Set\n",
    "\n",
    "**So... it looks like our fancy time-traveling walk-forward split was a bit too precise. Sometimes it leaves us with a validation set so small that our model can't even get a proper workout on it. It's like trying to test-drive a car by only moving it one inch.**\n",
    "\n",
    "This little function is our emergency duct tape.\n",
    "\n",
    "It throws elegance out the window and does one simple thing: it carves off a big, guaranteed chunk of our data (say, the last 30%) to create a fat validation set. It's not as clever as our previous method, but it ensures we have more than enough data to get a reliable score and prevent our training pipeline from complaining.\n",
    "\n",
    "Sometimes, you just need a bigger hammer. This is that hammer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f80be15-ec73-4040-8a81-103d63ef2c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split created: 73500 train, 31501 val samples\n",
      "Fixed split: Train 73500, Val 31501 samples\n"
     ]
    }
   ],
   "source": [
    "# Emergency fix for validation data\n",
    "def create_larger_validation_split(data, train_ratio=0.8, lookback=48, horizon=12):\n",
    "    \"\"\"Create a simple train/test split with enough validation data\"\"\"\n",
    "    total_samples = len(data)\n",
    "    split_point = int(total_samples * train_ratio)\n",
    "    \n",
    "    train_indices = np.arange(0, split_point)\n",
    "    val_indices = np.arange(split_point, total_samples)\n",
    "    \n",
    "    print(f\"Split created: {len(train_indices)} train, {len(val_indices)} val samples\")\n",
    "    \n",
    "    # Ensure validation has enough data\n",
    "    min_needed = lookback + horizon\n",
    "    if len(val_indices) >= min_needed:\n",
    "        return [(train_indices, val_indices)]\n",
    "    else:\n",
    "        print(f\"Warning: Validation needs {min_needed} samples, only has {len(val_indices)}\")\n",
    "        return None\n",
    "\n",
    "# Test the fix\n",
    "if 'featured_df' in globals() and not featured_df.empty:\n",
    "    test_splits = create_larger_validation_split(featured_df, train_ratio=0.7)\n",
    "    if test_splits:\n",
    "        train_idx, val_idx = test_splits[0]\n",
    "        print(f\"Fixed split: Train {len(train_idx)}, Val {len(val_idx)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e75fbf-229c-4ea4-b38d-05bf2ce3516d",
   "metadata": {},
   "source": [
    "# Interactive User Interface\n",
    "\n",
    "### üéÆ The Command Center: You're in Control Now\n",
    "\n",
    "*We've built all the individual pieces of our smart grid pipeline: the forecaster, the scenario generator, the optimizer, and the reporter. Now, it's time to put you in the driver's seat.*\n",
    "\n",
    "This final, glorious block uses ipywidgets to wrap our entire project in a sleek, interactive dashboard. It's no longer just a script; it's a tool you can play with.\n",
    "\n",
    "**How It Works**\n",
    "* **The Control Panel:** You'll see a set of sliders and dropdowns. These are your levers of power. Want to see what 10,000 EVs would do? Drag the slider. Want to change the off-peak charging window? Tweak the hours.\n",
    "\n",
    "* **The Big Green Button:** When you're ready, hit 'Run Full Pipeline'. This one click triggers the entire end-to-end process: it loads the data, trains the model, creates your custom EV apocalypse, optimizes it, and generates the final report based on your exact settings.\n",
    "\n",
    "* **The Main Screen:** All the output‚Äîfrom the training logs to the final dashboard charts‚Äîwill appear right below the button. You can run new experiments as many times as you like without ever having to touch the code again.\n",
    "\n",
    "Go ahead, play the role of a grid operator and see if you can find the optimal strategy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3013f36-059d-4fbd-9f43-568ab54d355e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Interactive User Interface ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397a245f3cc64d4c836202385418c42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>üîã Smart Grid Interactive Dashboard</h2>'), HTML(value='<p><em>70/30 train/test ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive interface ready. Adjust parameters and click 'Run Full Pipeline'.\n"
     ]
    }
   ],
   "source": [
    "if config.env_flags.get('USE_IPYWIDGETS', False):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    def validate_ui_inputs(ev_count, charging_power, charging_duration, start_hour, end_hour):\n",
    "        warnings = []\n",
    "        if ev_count <= 0:\n",
    "            warnings.append(\"Number of EVs should be > 0\")\n",
    "        if charging_power <= 0:\n",
    "            warnings.append(\"Charging power should be > 0 kW\")\n",
    "        if charging_duration <= 0 or charging_duration > 24:\n",
    "            warnings.append(\"Charging duration must be between 1 and 24 hours\")\n",
    "        if start_hour == end_hour:\n",
    "            warnings.append(\"Off-peak start and end hour are the same, window invalid\")\n",
    "        return warnings\n",
    "\n",
    "    def create_interactive_interface():\n",
    "        \"\"\"Creates an interactive interface for running the pipeline with different parameters.\"\"\"\n",
    "        \n",
    "        if not check_pipeline_state(\"Interactive interface creation\"):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Widget definitions\n",
    "            dataset_selector = widgets.Dropdown(\n",
    "                options=[('OPSD Germany', 'opsd'), ('UCI Household', 'uci_household')],\n",
    "                value='opsd',\n",
    "                description='Dataset:',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "            \n",
    "            ev_slider = widgets.IntSlider(value=1000, min=0, max=10000, step=500,\n",
    "                                          description='Number of EVs:', style={'description_width': 'initial'})\n",
    "            \n",
    "            charging_power_slider = widgets.FloatSlider(value=7.2, min=3.0, max=22.0, step=0.1,\n",
    "                                                        description='Charging Power (kW):', style={'description_width': 'initial'})\n",
    "            \n",
    "            charging_duration_slider = widgets.IntSlider(value=5, min=1, max=24, step=1,\n",
    "                                                         description='Charging duration (h):', style={'description_width': 'initial'})\n",
    "            \n",
    "            charging_window_start = widgets.IntSlider(value=22, min=0, max=23,\n",
    "                                                      description='Off-peak Start (hour):', style={'description_width': 'initial'})\n",
    "            \n",
    "            charging_window_end = widgets.IntSlider(value=7, min=0, max=23,\n",
    "                                                    description='Off-peak End (hour):', style={'description_width': 'initial'})\n",
    "            \n",
    "            run_button = widgets.Button(description='Run Full Pipeline', button_style='success',\n",
    "                                        tooltip='Train model, generate scenarios, and optimize', icon='play')\n",
    "            \n",
    "            output_area = widgets.Output()\n",
    "            \n",
    "            summary_panel = widgets.HTML()\n",
    "            \n",
    "            def on_run_button_clicked(b):\n",
    "                with output_area:\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "                    # Validate inputs\n",
    "                    warnings = validate_ui_inputs(ev_slider.value, charging_power_slider.value,\n",
    "                                                  charging_duration_slider.value,\n",
    "                                                  charging_window_start.value, charging_window_end.value)\n",
    "                    if warnings:\n",
    "                        for w in warnings:\n",
    "                            print(f\"‚ö†Ô∏è {w}\")\n",
    "                        return\n",
    "                    \n",
    "                    try:\n",
    "                        print(\"üöÄ Starting interactive pipeline run...\")\n",
    "                        print(\"=\" * 50)\n",
    "                        \n",
    "                        # Display summary of UI parameters\n",
    "                        summary_panel.value = f\"\"\"\n",
    "                        <b>Input Summary:</b><br>\n",
    "                        EVs: {ev_slider.value} | Power: {charging_power_slider.value:.1f} kW | Duration: {charging_duration_slider.value}h<br>\n",
    "                        Off-peak: {charging_window_start.value}:00 ‚Üí {charging_window_end.value}:00\n",
    "                        \"\"\"\n",
    "                        display(summary_panel)\n",
    "                        \n",
    "                        # Set fixed random seeds for reproducibility\n",
    "                        np.random.seed(42)\n",
    "                        torch.manual_seed(42)\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.manual_seed_all(42)\n",
    "                        \n",
    "                        # Reduced windows for validation\n",
    "                        temp_lookback = 48\n",
    "                        temp_horizon = 12\n",
    "                        print(f\"Using reduced windows: {temp_lookback}h lookback, {temp_horizon}h forecast\")\n",
    "                        \n",
    "                        global pipeline_state\n",
    "                        pipeline_state = PipelineState()\n",
    "                        \n",
    "                        # Load dataset\n",
    "                        if dataset_selector.value == 'opsd':\n",
    "                            raw_df = load_opsd()\n",
    "                            raw_target_col = 'Consumption'\n",
    "                        else:\n",
    "                            raw_df = load_uci_household()\n",
    "                            raw_target_col = 'Global_active_power'\n",
    "                        \n",
    "                        if raw_df.empty or raw_target_col not in raw_df.columns:\n",
    "                            print(f\"‚ùå Failed to load {dataset_selector.value} dataset\")\n",
    "                            return\n",
    "                        \n",
    "                        df = raw_df.rename(columns={raw_target_col: config.TARGET_COLUMN})\n",
    "                        print(f\"‚úÖ Loaded {dataset_selector.value} dataset: {df.shape}\")\n",
    "                        \n",
    "                        processed_df, scaler = preprocess_for_model(df, config.TARGET_COLUMN)\n",
    "                        if processed_df.empty:\n",
    "                            print(\"‚ùå Preprocessing failed\")\n",
    "                            return\n",
    "                        featured_df = build_features(processed_df, f\"{config.TARGET_COLUMN}_scaled\")\n",
    "                        if featured_df.empty:\n",
    "                            print(\"‚ùå Feature engineering failed\")\n",
    "                            return\n",
    "                        print(f\"‚úÖ Data preprocessed: {featured_df.shape}\")\n",
    "                        \n",
    "                        # 70/30 train/test split\n",
    "                        total_samples = len(featured_df)\n",
    "                        split_point = int(total_samples * 0.7)\n",
    "                        train_idx = np.arange(0, split_point)\n",
    "                        val_idx = np.arange(split_point, total_samples)\n",
    "                        train_data = featured_df.iloc[train_idx]\n",
    "                        val_data = featured_df.iloc[val_idx]\n",
    "                        print(f\"Train: {len(train_data)} | Val: {len(val_data)}\")\n",
    "                        \n",
    "                        min_needed = temp_lookback + temp_horizon\n",
    "                        if len(val_data) < min_needed:\n",
    "                            print(f\"‚ùå Insufficient validation data: need {min_needed}, have {len(val_data)}\")\n",
    "                            return\n",
    "                        \n",
    "                        # Initialize model\n",
    "                        target_col_scaled = f\"{config.TARGET_COLUMN}_scaled\"\n",
    "                        feature_cols = [c for c in featured_df.columns if c not in [config.TARGET_COLUMN, target_col_scaled]]\n",
    "                        model = TCNTransformerHybrid(input_size=len(feature_cols), output_size=1,\n",
    "                                                     horizon=temp_horizon, num_quantiles=len(config.QUANTILES))\n",
    "                        print(f\"‚úÖ Model initialized with {len(feature_cols)} features\")\n",
    "                        \n",
    "                        import copy\n",
    "                        temp_config = copy.deepcopy(config)\n",
    "                        temp_config.LOOKBACK_WINDOW = temp_lookback\n",
    "                        temp_config.FORECAST_HORIZON = temp_horizon\n",
    "                        \n",
    "                        # Train model\n",
    "                        trained_model, history = train_pytorch_model(model, train_data, val_data, temp_config, feature_cols)\n",
    "                        if trained_model is None:\n",
    "                            print(\"‚ùå Model training failed\")\n",
    "                            return\n",
    "                        print(\"‚úÖ Model training completed\")\n",
    "                        \n",
    "                        # Generate predictions\n",
    "                        X_val = history.get('X_val', np.array([]))\n",
    "                        y_val = history.get('y_val', np.array([]))\n",
    "                        if X_val.shape[0] == 0:\n",
    "                            print(\"‚ö†Ô∏è No validation data available for predictions\")\n",
    "                            return\n",
    "                        device = torch.device(\"cuda\" if config.env_flags.get('CUDA_AVAILABLE', False) else \"cpu\")\n",
    "                        trained_model.to(device)\n",
    "                        trained_model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            predictions = trained_model(torch.from_numpy(X_val).to(device)).cpu().numpy()\n",
    "                        print(f\"‚úÖ Generated predictions: {predictions.shape}\")\n",
    "                        \n",
    "                        p50_idx = config.QUANTILES.index(0.5) if 0.5 in config.QUANTILES else len(config.QUANTILES)//2\n",
    "                        baseline_scaled = predictions[0, :, p50_idx]\n",
    "                        if isinstance(scaler, dict):\n",
    "                            baseline_forecast = baseline_scaled * scaler['std'] + scaler['mean']\n",
    "                        else:\n",
    "                            baseline_forecast = scaler.inverse_transform(baseline_scaled.reshape(-1, 1)).flatten()\n",
    "                        \n",
    "                        # Forecast index\n",
    "                        forecast_start_idx = temp_lookback\n",
    "                        forecast_end_idx = forecast_start_idx + temp_horizon\n",
    "                        if len(val_data) >= forecast_end_idx:\n",
    "                            forecast_index = val_data.index[forecast_start_idx:forecast_end_idx]\n",
    "                        else:\n",
    "                            start_time = val_data.index[0] if len(val_data) > 0 else pd.Timestamp.now()\n",
    "                            forecast_index = pd.date_range(start_time, periods=temp_horizon, freq='H')\n",
    "                        print(f\"‚úÖ Forecast index: {len(forecast_index)} periods ({forecast_index[0]} ‚Üí {forecast_index[-1]})\")\n",
    "                        \n",
    "                        # Generate EV load\n",
    "                        ev_profile = generate_ev_load_profile(\n",
    "                            num_evs=ev_slider.value,\n",
    "                            charging_power_kw=charging_power_slider.value,\n",
    "                            charging_duration_h=charging_duration_slider.value,\n",
    "                            horizon_h=temp_horizon,\n",
    "                            baseline_index=forecast_index\n",
    "                        )\n",
    "                        scenario_data = create_scenario_dataframe(baseline_forecast, forecast_index, ev_profile)\n",
    "                        if scenario_data.empty:\n",
    "                            print(\"‚ùå Failed to create scenario\")\n",
    "                            return\n",
    "                        print(\"‚úÖ Scenario generated successfully\")\n",
    "                        \n",
    "                        # Run optimizer\n",
    "                        optimized_load, shifted_ev = greedy_load_shifter(\n",
    "                            scenario_data['baseline_load'],\n",
    "                            scenario_data['ev_load'],\n",
    "                            charging_window=(charging_window_start.value, charging_window_end.value)\n",
    "                        )\n",
    "                        scenario_data['optimized_total_load'] = optimized_load\n",
    "                        scenario_data['shifted_ev_load'] = shifted_ev\n",
    "                        print(\"‚úÖ Optimization completed\")\n",
    "                        \n",
    "                        # Report\n",
    "                        report_metrics = generate_advanced_report(scenario_data)\n",
    "                        if report_metrics:\n",
    "                            print(\"\\n\" + \"=\"*50)\n",
    "                            print(\"üìä INTERACTIVE RUN SUMMARY\")\n",
    "                            print(\"=\"*50)\n",
    "                            print(f\"Dataset: {dataset_selector.value.upper()}\")\n",
    "                            print(f\"Model Config: {temp_lookback}h lookback, {temp_horizon}h forecast\")\n",
    "                            print(f\"EVs: {ev_slider.value} | Power: {charging_power_slider.value:.1f} kW | Duration: {charging_duration_slider.value}h\")\n",
    "                            print(f\"Off-peak Window: {charging_window_start.value}:00 ‚Üí {charging_window_end.value}:00\")\n",
    "                            print(\"=\"*50)\n",
    "                        else:\n",
    "                            print(\"‚ö†Ô∏è Report generation had issues but optimization completed\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error during interactive run: {str(e)}\")\n",
    "                        import traceback\n",
    "                        print(traceback.format_exc())\n",
    "            \n",
    "            run_button.on_click(on_run_button_clicked)\n",
    "            \n",
    "            # Layout\n",
    "            parameter_controls = widgets.VBox([\n",
    "                widgets.HTML(\"<h3>Dataset Selection</h3>\"),\n",
    "                dataset_selector,\n",
    "                widgets.HTML(\"<h3>EV Scenario Parameters</h3>\"),\n",
    "                widgets.HBox([ev_slider, charging_power_slider, charging_duration_slider]),\n",
    "                widgets.HTML(\"<h3>Optimization Settings</h3>\"),\n",
    "                widgets.HBox([charging_window_start, charging_window_end]),\n",
    "                widgets.HTML(\"<small>Off-peak window: start hour ‚Üí end hour (next day if end < start)</small>\")\n",
    "            ])\n",
    "            \n",
    "            interface = widgets.VBox([\n",
    "                widgets.HTML(\"<h2>üîã Smart Grid Interactive Dashboard</h2>\"),\n",
    "                widgets.HTML(\"<p><em>70/30 train/test split with 48h lookback + 12h forecast</em></p>\"),\n",
    "                parameter_controls,\n",
    "                run_button,\n",
    "                output_area\n",
    "            ])\n",
    "            \n",
    "            return interface\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating interactive interface: {str(e)}\")\n",
    "            return widgets.HTML(\"<p>Error: Could not create interactive interface</p>\")\n",
    "    \n",
    "    print(\"--- Interactive User Interface ---\")\n",
    "    try:\n",
    "        ui = create_interactive_interface()\n",
    "        if ui is not None:\n",
    "            display(ui)\n",
    "            print(\"‚úÖ Interactive interface ready. Adjust parameters and click 'Run Full Pipeline'.\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to create interactive interface\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying interface: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Interactive UI not available - ipywidgets not found\")\n",
    "    print(\"üí° Install with: pip install ipywidgets\")\n",
    "    print(\"üìù Alternative: Use CLI or run cells manually\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
